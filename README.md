




# Abstract
 This thesis explores the application of the transformer architecture to the problem of competitive snake games. We explore various strategies for training the transformers, from the creation of training batches to corpora development and tokenization. Furthermore, we examine how different algorithms influence the model's training and identify the most effective approaches for embedding algorithmic behaviors into the transformer's weights. Finally, by comparing our specialized models with a state-of-the-art Large Language Model (LLM), we demonstrate that a purpose-trained Transformer is more suitable for this specific task than a general LLM.


This paper explores different ways the transformer architecture can be utilized as an artificial intelligence agent for playing competitive snake games. We explore various training strategies, beginning with training on full game records, examining different sampling methods, and concluding with the creation of weighted masks on training data, which helped the transformer focus on key aspects of the given data. Furthermore, we explore how dataset verbosity influences the transformer's gameplay quality and how effectively the transformer embeds different AI algorithm behaviors into its weights, by comparing our trained transformer with the AI algorithm agents that created the training datasets for the model. Finally, by comparing our specialized models with a state-of-the-art Large Language Model (LLM), we demonstrate that a purpose-trained transformer is more suitable for this specific task than a general LLM.


Exploring autoregressive learaning models capabilities playing simple arcade game - Snake, with succinct game state provided.



This paper explores different ways the transformer architecture can be utilized as an artificial intelligence agent for playing competitive snake games. We train the transformer on multiple datasets generated by  different classic AI Agents algorithms. We explore various training strategies, beginning with training on succint game-state records, examining different sampling methods, and concluding with the creation of weighted masks on training data, which helped the transformer focus on key aspects of the given data. Furthermore, we explore how dataset verbosity influences the transformer's gameplay quality and how effectively the transformer embeds different AI algorithm behaviors into its weights, by comparing our trained transformer with the AI algorithm agents that created the training datasets for the model. Finally, by comparing our specialized models with a state-of-the-art Large Language Model (LLM), we demonstrate that a purpose-trained transformer is more suitable for this specific task than a general LLM.

# Link for the thesis file
https://typst.app/project/r8PxTskiUavht0lQhxPVfQ
